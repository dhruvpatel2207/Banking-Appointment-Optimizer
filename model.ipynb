{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correctly format the file path\n",
    "file_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\Healthcare\\\\healthcare dataset.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime format, ensuring errors are handled as NaT for invalid entries\n",
    "df['Admission Date'] = pd.to_datetime(df['Admission Date'], format='%d-%m-%Y', errors='coerce')\n",
    "df['Discharge Date'] = pd.to_datetime(df['Discharge Date'], format='%d-%m-%Y', errors='coerce')\n",
    "df['Scheduled Date'] = pd.to_datetime(df['Scheduled Date'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "# Check if there are any invalid or missing values in 'Scheduled Date'\n",
    "if df['Scheduled Date'].isnull().sum() > 0:\n",
    "    print(f\"Warning: Some 'Scheduled Date' entries are invalid and have been set to NaT.\")\n",
    "    print(f\"Total invalid 'Scheduled Date' entries: {df['Scheduled Date'].isnull().sum()}\")\n",
    "    # Optionally, forward fill or use other imputation methods\n",
    "    df['Scheduled Date'] = df['Scheduled Date'].fillna(method='bfill')  # Backward fill\n",
    "\n",
    "# Reference date for converting Scheduled Date to numeric (days since reference date)\n",
    "reference_date = pd.to_datetime('2020-01-01')\n",
    "\n",
    "# Calculate the number of days between the reference date and the Scheduled Date\n",
    "df['Scheduled Days'] = (df['Scheduled Date'] - reference_date).dt.days\n",
    "\n",
    "# Check if 'Scheduled Days' has been calculated correctly\n",
    "if 'Scheduled Days' not in df.columns:\n",
    "    raise ValueError(\"'Scheduled Days' column is missing after calculation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: 2 unique values\n",
      "Blood Type: 8 unique values\n",
      "Medical Condition: 6 unique values\n",
      "Admission Type: 3 unique values\n"
     ]
    }
   ],
   "source": [
    "# Example of creating additional feature: Days since admission\n",
    "df['Days Since Admission'] = (df['Scheduled Date'] - df['Admission Date']).dt.days\n",
    "\n",
    "# Handle missing values in the new feature\n",
    "df['Days Since Admission'] = df['Days Since Admission'].fillna(0)  # Replace NaN with 0\n",
    "df['Days Since Admission'] = df['Days Since Admission'].clip(lower=0)  # Ensure no negative days\n",
    "\n",
    "# One-hot encoding categorical features like Gender, Blood Type, Medical Condition, Admission Type\n",
    "# Check cardinality before encoding\n",
    "categorical_cols = ['Gender', 'Blood Type', 'Medical Condition', 'Admission Type']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        unique_values = df[col].nunique()\n",
    "        print(f\"{col}: {unique_values} unique values\")\n",
    "        if unique_values > 20:  # Threshold for high cardinality\n",
    "            print(f\"High cardinality detected in {col}. Consider alternative encodings.\")\n",
    "        else:\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the target column and any non-relevant columns like 'Name', 'Patient_ID', 'Admission Date', 'Discharge Date'\n",
    "columns_to_drop = ['Name', 'Patient_ID', 'Admission Date', 'Discharge Date']\n",
    "existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "df = df.drop(columns=existing_columns, errors='ignore')  # Drop non-relevant columns\n",
    "\n",
    "# Define the target variable 'y' as 'Scheduled Date'\n",
    "if 'Scheduled Date' in df.columns:\n",
    "    y = df['Scheduled Date']  # Target variable\n",
    "    X = df.drop(columns=['Scheduled Date'], errors='ignore')  # Drop target column from features\n",
    "else:\n",
    "    raise KeyError(\"'Scheduled Date' column is missing, so it cannot be used as the target variable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns in X: ['Medication', 'Test Results', 'Hospital', 'Email']\n",
      "Medication: 5 unique values\n",
      "Test Results: 3 unique values\n",
      "Hospital: 2055 unique values\n",
      "Skipping one-hot encoding for Hospital due to high cardinality.\n",
      "Email: 476 unique values\n",
      "Skipping one-hot encoding for Email due to high cardinality.\n",
      "Size after filling missing values and encoding: (2110, 28)\n",
      "Training set size: (1688, 28)\n",
      "Test set size: (422, 28)\n"
     ]
    }
   ],
   "source": [
    "# Identify already encoded columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Non-numeric columns in X:\", non_numeric_columns)\n",
    "\n",
    "# Check for high cardinality and apply encoding only once\n",
    "for col in non_numeric_columns:\n",
    "    unique_values = X[col].nunique()\n",
    "    print(f\"{col}: {unique_values} unique values\")\n",
    "    if unique_values > 20:  # Threshold for high cardinality\n",
    "        print(f\"Skipping one-hot encoding for {col} due to high cardinality.\")\n",
    "        # Optional: Use frequency or target encoding instead\n",
    "        X[col + '_freq'] = X[col].map(X[col].value_counts())\n",
    "        X = X.drop(columns=[col], errors='ignore')\n",
    "    else:\n",
    "        X = pd.get_dummies(X, columns=[col], drop_first=True)\n",
    "\n",
    "# Fill missing numerical values with the median (after encoding)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Ensure the target variable 'y' has no missing values (optional, depends on your target)\n",
    "y = y.fillna(y.median())  # Replace with `.mode()[0]` if `y` is categorical (for mode-based filling)\n",
    "\n",
    "# Check the shape after filling missing values and encoding categorical variables\n",
    "print(\"Size after filling missing values and encoding:\", X.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the size of training and testing sets\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender_Male: 2 unique values\n",
      "Blood Type_A-: 2 unique values\n",
      "Blood Type_AB+: 2 unique values\n",
      "Blood Type_AB-: 2 unique values\n",
      "Blood Type_B+: 2 unique values\n",
      "Blood Type_B-: 2 unique values\n",
      "Blood Type_O+: 2 unique values\n",
      "Blood Type_O-: 2 unique values\n",
      "Medical Condition_Asthma: 2 unique values\n",
      "Medical Condition_Cancer: 2 unique values\n",
      "Medical Condition_Diabetes: 2 unique values\n",
      "Medical Condition_Hypertension: 2 unique values\n",
      "Medical Condition_Obesity: 2 unique values\n",
      "Admission Type_Emergency: 2 unique values\n",
      "Admission Type_Urgent: 2 unique values\n",
      "Medication_Ibuprofen: 2 unique values\n",
      "Medication_Lipitor: 2 unique values\n",
      "Medication_Paracetamol: 2 unique values\n",
      "Medication_Penicillin: 2 unique values\n",
      "Test Results_Inconclusive: 2 unique values\n",
      "Test Results_Normal: 2 unique values\n",
      "No specific columns available for one-hot encoding.\n",
      "Shape of X after encoding: (2110, 28)\n",
      "Training data shape: (1688, 28)\n",
      "Test data shape: (422, 28)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'X' is your feature DataFrame and 'y' is your target variable\n",
    "\n",
    "# Step 1: Handle missing values for numerical columns by filling with the median\n",
    "numerical_columns = X.select_dtypes(include=['number']).columns\n",
    "X[numerical_columns] = X[numerical_columns].fillna(X[numerical_columns].median())\n",
    "\n",
    "# Step 2: Handle missing values for categorical columns by filling with the mode (most frequent value)\n",
    "categorical_columns = X.select_dtypes(exclude=['number']).columns\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].fillna(X[col].mode()[0])\n",
    "\n",
    "# Step 3: Handle missing values for categorical columns and apply one-hot encoding or frequency encoding\n",
    "for col in categorical_columns:\n",
    "    if col in X.columns:\n",
    "        unique_values = X[col].nunique()\n",
    "        print(f\"{col}: {unique_values} unique values\")\n",
    "        if unique_values > 20:  # High cardinality threshold\n",
    "            print(f\"Skipping one-hot encoding for {col} due to high cardinality. Applying frequency encoding instead.\")\n",
    "            X[col + '_freq'] = X[col].map(X[col].value_counts())\n",
    "            X = X.drop(columns=[col], errors='ignore')  # Drop original column\n",
    "        else:\n",
    "            X = pd.get_dummies(X, columns=[col], drop_first=True)\n",
    "\n",
    "# Step 4: Identify specific columns for one-hot encoding that exist in the DataFrame\n",
    "one_hot_columns = ['Medication', 'Test Results', 'Hospital']  # Desired columns for one-hot encoding\n",
    "existing_one_hot_columns = [col for col in one_hot_columns if col in X.columns]\n",
    "\n",
    "# Step 5: Apply one-hot encoding only to the specific existing columns\n",
    "if existing_one_hot_columns:\n",
    "    X = pd.get_dummies(X, columns=existing_one_hot_columns, drop_first=True)\n",
    "else:\n",
    "    print(\"No specific columns available for one-hot encoding.\")\n",
    "\n",
    "# Step 6: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the resulting dataset to confirm the changes\n",
    "print(\"Shape of X after encoding:\", X.shape)\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (1688, 28)\n",
      "Test set size: (422, 28)\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset has enough rows for splitting\n",
    "if X.shape[0] < 10:  # This ensures that you have at least 10 rows in the dataset\n",
    "    raise ValueError(\"Not enough data to perform train-test split\")\n",
    "\n",
    "# After splitting, check if both the training and testing sets have a reasonable number of rows\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "if X_train.shape[0] < 5 or X_test.shape[0] < 5:\n",
    "    raise ValueError(\"Train-test split resulted in too few samples in either the training or testing set.\")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after dropping unnecessary columns: ['Age', 'Medication', 'Test Results', 'Scheduled Date', 'Hospital', 'Billing Amount', 'Email', 'Contact Number', 'Scheduled Days', 'Days Since Admission', 'Gender_Male', 'Blood Type_A-', 'Blood Type_AB+', 'Blood Type_AB-', 'Blood Type_B+', 'Blood Type_B-', 'Blood Type_O+', 'Blood Type_O-', 'Medical Condition_Asthma', 'Medical Condition_Cancer', 'Medical Condition_Diabetes', 'Medical Condition_Hypertension', 'Medical Condition_Obesity', 'Admission Type_Emergency', 'Admission Type_Urgent']\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns: 'Name' and 'Patient_ID'\n",
    "columns_to_drop = ['Name', 'Patient_ID']\n",
    "existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "# Drop the columns if they exist\n",
    "df = df.drop(columns=existing_columns, errors='ignore')  # 'ignore' ensures no error if columns are missing\n",
    "\n",
    "# Check if the columns have been removed\n",
    "print(\"Columns after dropping unnecessary columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in 'Scheduled Date' after filling: 0\n",
      "Null values in 'Admission Date' after filling: 0\n",
      "High-cardinality categorical columns: ['Name', 'Hospital', 'Email']\n",
      "Low-cardinality categorical columns: ['Gender', 'Blood Type', 'Medical Condition', 'Admission Type', 'Medication', 'Test Results']\n",
      "Feature set shape: (2110, 33)\n",
      "Target variable shape: (2110,)\n",
      "Best Hyperparameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean Absolute Error (MAE): 1.6623044398158648\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor  # Import XGBoost\n",
    "\n",
    "# Correctly format the file path\n",
    "file_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\Healthcare\\\\healthcare dataset.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert date columns to datetime format, use 'coerce' to convert invalid dates to NaT\n",
    "df['Admission Date'] = pd.to_datetime(df['Admission Date'], format='%d-%m-%Y', errors='coerce')\n",
    "df['Discharge Date'] = pd.to_datetime(df['Discharge Date'], format='%d-%m-%Y', errors='coerce')\n",
    "df['Scheduled Date'] = pd.to_datetime(df['Scheduled Date'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "# Handle missing values in date columns (forward fill)\n",
    "df['Scheduled Date'] = df['Scheduled Date'].ffill()\n",
    "df['Admission Date'] = df['Admission Date'].ffill()\n",
    "\n",
    "# Recheck for NaT values\n",
    "print(f\"Null values in 'Scheduled Date' after filling: {df['Scheduled Date'].isnull().sum()}\")\n",
    "print(f\"Null values in 'Admission Date' after filling: {df['Admission Date'].isnull().sum()}\")\n",
    "\n",
    "# Feature Engineering: Create Year, Month, Day features from dates\n",
    "df['Admission Year'] = df['Admission Date'].dt.year\n",
    "df['Admission Month'] = df['Admission Date'].dt.month\n",
    "df['Admission Day'] = df['Admission Date'].dt.day\n",
    "\n",
    "df['Scheduled Year'] = df['Scheduled Date'].dt.year\n",
    "df['Scheduled Month'] = df['Scheduled Date'].dt.month\n",
    "df['Scheduled Day'] = df['Scheduled Date'].dt.day\n",
    "\n",
    "# Create a 'Days Since Admission' feature based on the difference in days between 'Scheduled Date' and 'Admission Date'\n",
    "df['Days Since Admission'] = (df['Scheduled Date'] - df['Admission Date']).dt.days\n",
    "\n",
    "# Fill any remaining missing values in 'Days Since Admission' with 0\n",
    "df['Days Since Admission'] = df['Days Since Admission'].fillna(0).astype('int64')\n",
    "\n",
    "# Handle other missing values in the dataset (other than date columns)\n",
    "# Using median for numerical columns and mode for categorical columns\n",
    "for col in df.select_dtypes(include=['number']).columns:\n",
    "    df[col] = df[col].fillna(df[col].median())  # For numerical columns\n",
    "\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])  # For categorical columns\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Check the number of unique values in each categorical column\n",
    "high_cardinality_cols = [col for col in categorical_cols if df[col].nunique() > 50]\n",
    "low_cardinality_cols = [col for col in categorical_cols if df[col].nunique() <= 50]\n",
    "\n",
    "print(f\"High-cardinality categorical columns: {high_cardinality_cols}\")\n",
    "print(f\"Low-cardinality categorical columns: {low_cardinality_cols}\")\n",
    "\n",
    "# Perform one-hot encoding only on low-cardinality columns\n",
    "if low_cardinality_cols:\n",
    "    df = pd.get_dummies(df, columns=low_cardinality_cols, drop_first=True)\n",
    "\n",
    "# For high-cardinality columns, use label encoding\n",
    "for col in high_cardinality_cols:\n",
    "    df[col] = df[col].astype('category').cat.codes  # Label encoding\n",
    "\n",
    "# Drop non-relevant columns (including original date columns)\n",
    "columns_to_drop = ['Name', 'Patient_ID', 'Admission Date', 'Discharge Date']  # Remove original date columns\n",
    "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Define the target variable (y) and feature set (X)\n",
    "# Ensure 'Scheduled Date' is in the dataset before using it as target\n",
    "if 'Scheduled Date' in df.columns:\n",
    "    y = df['Scheduled Date'].values  # Use the original Scheduled Date as target variable\n",
    "    reference_date = pd.to_datetime('2020-01-01')  # Reference date to convert Scheduled Date to number of days\n",
    "    y = (df['Scheduled Date'] - reference_date).dt.days.values  # Convert 'Scheduled Date' to number of days\n",
    "    X = df.drop(columns=['Scheduled Date'], errors='ignore')  # Drop 'Scheduled Date' column from features\n",
    "else:\n",
    "    raise KeyError(\"'Scheduled Date' column is missing and cannot be used as the target variable.\")\n",
    "\n",
    "# Check for NaN or infinite values in the dataset\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"Data contains NaN values. Filling them with the median.\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "if np.any(np.isinf(X)):\n",
    "    print(\"Data contains infinite values. Replacing them with 0.\")\n",
    "    X.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "# Check if features and target are correctly defined\n",
    "print(\"Feature set shape:\", X.shape)\n",
    "print(\"Target variable shape:\", y.shape)  # This should display (number of rows,)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning (Optional) using GridSearchCV with a simplified grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Number of boosting rounds\n",
    "    'max_depth': [3, 6],  # Maximum depth of the trees\n",
    "    'learning_rate': [0.01, 0.1],  # Learning rate\n",
    "    'subsample': [0.8, 1.0],  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': [0.8, 1.0]  # Subsample ratio of columns when constructing each tree\n",
    "}\n",
    "\n",
    "# GridSearchCV for XGBRegressor\n",
    "grid_search = GridSearchCV(estimator=XGBRegressor(random_state=42), param_grid=param_grid, cv=3, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters found\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Initialize and train the model with the best hyperparameters\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=grid_search.best_params_['n_estimators'],\n",
    "    max_depth=grid_search.best_params_['max_depth'],\n",
    "    learning_rate=grid_search.best_params_['learning_rate'],\n",
    "    subsample=grid_search.best_params_['subsample'],\n",
    "    colsample_bytree=grid_search.best_params_['colsample_bytree'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.257096634091924\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1-Score   AUC-ROC\n",
      "0  Logistic Regression  0.850000   0.876712  0.825806  0.850498  0.914171\n",
      "1        Random Forest  0.863333   0.895833  0.832258  0.862876  0.923915\n",
      "2                  SVM  0.833333   0.857143  0.812903  0.834437  0.911190\n",
      "3    Gradient Boosting  0.866667   0.885906  0.851613  0.868421  0.923471\n",
      "4        Decision Tree  0.846667   0.881119  0.812903  0.845638  0.847831\n",
      "5  K-Nearest Neighbors  0.803333   0.847826  0.754839  0.798635  0.873571\n",
      "6          Naive Bayes  0.813333   0.877863  0.741935  0.804196  0.885206\n",
      "7              XGBoost  0.896667   0.942857  0.851613  0.894915  0.943404\n",
      "\n",
      "Best Model:\n",
      "Model         XGBoost\n",
      "Accuracy     0.896667\n",
      "Precision    0.942857\n",
      "Recall       0.851613\n",
      "F1-Score     0.894915\n",
      "AUC-ROC      0.943404\n",
      "Name: 7, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:17:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# XGBClassifier without the deprecated parameter\n",
    "xgb_model = XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "\n",
    "# Example dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    # \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    # \"LightGBM\": LGBMClassifier()\n",
    "}\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = []\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan\n",
    "    \n",
    "    # Append to results\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"AUC-ROC\": auc_roc\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)\n",
    "\n",
    "# Select the best model based on F1-Score\n",
    "best_model = results_df.loc[results_df['F1-Score'].idxmax()]\n",
    "print(\"\\nBest Model:\")\n",
    "print(best_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
